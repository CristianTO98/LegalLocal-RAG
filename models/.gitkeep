# ==============================================================================
# LegalLocal RAG - Models Directory
# ==============================================================================
#
# Place your GGUF model files in this directory.
#
# RECOMMENDED MODELS (Download from HuggingFace):
# ───────────────────────────────────────────────
#
# For Speed Mode (Llama 3.2 3B):
#   - Filename: Llama-3.2-3B-Instruct-Q4_K_M.gguf
#   - Size: ~2 GB
#   - RAM Required: ~2.5 GB during inference
#   - Source: https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-GGUF
#
# For Precision Mode (Llama 3.1 8B):
#   - Filename: Llama-3.1-8B-Instruct-Q4_K_M.gguf
#   - Size: ~4.5 GB
#   - RAM Required: ~5 GB during inference
#   - Source: https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF
#
# QUANTIZATION GUIDE:
# ───────────────────
# Q4_K_M - Recommended (Best quality/size balance)
# Q4_K_S - Smaller, slightly lower quality
# Q5_K_M - Higher quality, larger size
# Q8_0   - Near original quality, much larger
#
# DOWNLOAD COMMAND (using huggingface-cli):
# huggingface-cli download bartowski/Llama-3.2-3B-Instruct-GGUF \
#   --include "Llama-3.2-3B-Instruct-Q4_K_M.gguf" \
#   --local-dir ./models
#
# ==============================================================================

