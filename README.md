# âš–ï¸ LegalLocal RAG

> **Air-Gapped Legal Research Assistant** - 100% Offline RAG System for Legal Professionals

[![Python 3.10+](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Privacy: Air-Gapped](https://img.shields.io/badge/Privacy-Air--Gapped-green.svg)]()

---

## ğŸ§  Por QuÃ© Este Sistema Es Diferente: La FilosofÃ­a DetrÃ¡s del DiseÃ±o

### El Problema del RAG Tradicional en Documentos Legales

Cuando un abogado pregunta *"Â¿CuÃ¡l es la multa por retraso en la entrega?"*, un sistema RAG tradicional cortarÃ­a el documento en trozos de tamaÃ±o fijo (por ejemplo, 500 caracteres). El problema es que ese corte puede partir una clÃ¡usula a la mitad.

**Imagina este escenario:** La respuesta relevante estÃ¡ en la ClÃ¡usula 8.3 que dice:

> *"En caso de fuerza mayor debidamente acreditada, el contratista quedarÃ¡ eximido de cualquier penalizaciÃ³n. Sin embargo, en caso de retraso injustificado, se aplicarÃ¡ una penalizaciÃ³n del 5% diario sobre el valor del contrato..."*

Con chunking tradicional, la frase *"se aplicarÃ¡ una penalizaciÃ³n del 5% diario"* podrÃ­a estar en un chunk, mientras que la excepciÃ³n de *"fuerza mayor"* quedÃ³ en el chunk anterior. El LLM te darÃ­a un consejo legal **errÃ³neo** porque nunca vio la excepciÃ³n.

### La SoluciÃ³n: Parent-Child Indexing con Structure-Aware Chunking

Este MVP implementa una estrategia de **indexaciÃ³n jerÃ¡rquica** diseÃ±ada especÃ­ficamente para documentos legales:

1. **Parents (Nodos Padre)**: Dividimos el documento respetando su estructura semÃ¡ntica natural. Los documentos legales tienen patrones claros: *"ArtÃ­culo X"*, *"ClÃ¡usula Y"*, *"1.1."*, *"1.2."*. Cada Parent es una unidad semÃ¡ntica completa (una clÃ¡usula, un artÃ­culo).

2. **Children (Nodos Hijo)**: Cada Parent se subdivide en trozos pequeÃ±os (~256 tokens) que son los que realmente se indexan y buscan.

3. **El Truco**: Cuando buscas, el sistema encuentra un Child muy especÃ­fico (alta precisiÃ³n en la bÃºsqueda). Pero cuando recupera el contexto para el LLM, **sube al Parent completo**. AsÃ­ el modelo siempre tiene la clÃ¡usula entera con todas sus excepciones y matices.

### Â¿Por QuÃ© el Modelo BGE-small para Embeddings?

Elegimos **BAAI/bge-small-en-v1.5** por razones muy especÃ­ficas:

| CaracterÃ­stica | Valor | Por QuÃ© Importa |
|----------------|-------|-----------------|
| **TamaÃ±o** | ~130 MB | Cabe en memoria sin problema, carga instantÃ¡nea |
| **Rendimiento** | State-of-the-Art | Supera a modelos mÃ¡s grandes en benchmarks de recuperaciÃ³n (MTEB) |
| **Contexto** | 512 tokens | Perfecto para los Child chunks de 256 tokens |
| **Velocidad CPU** | Optimizado | Latencia imperceptible en laptops de oficina |

> ğŸ’¡ **Nota tÃ©cnica**: BGE requiere un prefijo especial para queries: *"Represent this sentence for searching relevant passages: "* â€” esto ya estÃ¡ implementado en el sistema.

### Â¿Por QuÃ© Qwen 3 4B con CuantizaciÃ³n Q4?

DespuÃ©s de probar mÃºltiples modelos (Qwen 3 4B thinking, Qwen 2.5 3B, Gemma 3n e4b Q4, Gemma 3 4B, Gemma 3n e4b Q8), **Qwen 3 4B Instruct** con cuantizaciÃ³n Q4_K_M demostrÃ³ ser el mejor balance entre:

- **Velocidad**: Mayor cantidad de tokens/segundo en CPU puro.
- **Inteligencia**: Respuestas coherentes y bien estructuradas para tareas legales, con capacidades superiores de razonamiento.
- **Consumo**: ~2.5 GB de RAM, ideal para portÃ¡tiles de oficina.

Este sistema estÃ¡ **diseÃ±ado para correr en cualquier portÃ¡til de oficina sin GPU**. No necesitas una mÃ¡quina gaming ni una workstation con CUDA. Un Ryzen 5 o Intel i5 con 16GB de RAM es mÃ¡s que suficiente.

---

## ğŸ¯ Overview

LegalLocal RAG is a **privacy-first** legal document analysis tool designed for professionals who need to process sensitive documents without any risk of data exposure. The system runs **100% offline** on standard laptop hardware (Ryzen 5/Intel i5, 16GB RAM, no GPU required).

### Key Features

- ğŸ”’ **True Air-Gap Compliance**: Zero network calls at runtime
- ğŸ§  **Qwen 3 4B**: State-of-the-art intelligence for CPU-based RAG
- ğŸ“„ **Smart PDF Analysis**: Structure-aware extraction with PyMuPDF
- ğŸ” **Parent-Child RAG**: Hierarchical indexing for legal document precision
- ğŸ’¬ **Modern Chat Interface**: Powered by Chainlit for a professional UX
- ğŸ“‘ **Side-by-Side Citations**: Click on references to see original context immediately
- ğŸ’» **CPU-Optimized**: Runs on standard office hardware without GPU

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LegalLocal RAG v2.0                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Chainlit   â”‚â”€â”€â”€â–¶â”‚  LlamaIndex  â”‚â”€â”€â”€â–¶â”‚  llama-cpp-python       â”‚ â”‚
â”‚  â”‚     UI      â”‚    â”‚ Orchestrator â”‚    â”‚  (Embedded LLM Engine)  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚         â”‚                  â”‚                        â”‚                â”‚
â”‚         â–¼                  â–¼                        â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   PyMuPDF   â”‚    â”‚   ChromaDB   â”‚    â”‚   BAAI/bge-small-en     â”‚ â”‚
â”‚  â”‚ (Block Ext.)â”‚    â”‚(Vector Store)â”‚    â”‚   (130MB Embeddings)    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              PARENT-CHILD HIERARCHICAL INDEXING                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Parent Chunks (1024-2048 tokens) - Full clauses/articles    â”‚  â”‚
â”‚  â”‚       â†“                                                       â”‚  â”‚
â”‚  â”‚  Child Chunks (256-512 tokens) - What gets indexed & searchedâ”‚  â”‚
â”‚  â”‚       â†“                                                       â”‚  â”‚
â”‚  â”‚  On retrieval: Child matches â†’ Return Parent for context     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    LOCAL MODEL (No API Calls)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  /models/Qwen3-4B-Instruct-2507-Q4_K_M.gguf  (~2.3 GB)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Requirements

### Hardware Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| CPU | 4 cores | 6+ cores (Ryzen 5/Intel i5) |
| RAM | 8 GB | 16 GB |
| Storage | 5 GB free | 10 GB free |
| GPU | **Not required** | **Not required** |

### Software Requirements

- Python 3.10 or higher
- Windows 10/11, macOS, or Linux

## ğŸš€ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/LegalLocal-RAG.git
cd LegalLocal-RAG
```

### 2. Create Virtual Environment

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Download the Model

Download the Qwen 3 4B GGUF model and place it in the `/models` directory:

```bash
# Example using huggingface-cli for Qwen 2.5 (as Qwen 3 is a custom file)
huggingface-cli download Qwen/Qwen2.5-3B-Instruct-GGUF \
  --include "Qwen2.5-3B-Instruct-Q4_K_M.gguf" \
  --local-dir ./models
```

> **Note**: The model file should be named `Qwen3-4B-Instruct-2507-Q4_K_M.gguf` in the models folder.

### 5. Run the Application

```bash
chainlit run app.py -w
```

The application will open in your default browser at `http://localhost:8000`

## ğŸ“– Usage Guide

### Step 1: Upload Document

Upload a PDF document using the sidebar uploader. The system will:
1. Extract text by blocks using PyMuPDF (faster, cleaner extraction)
2. Create Parent chunks respecting document structure (articles, clauses)
3. Split Parents into Child chunks for precise indexing
4. Generate BGE embeddings for all Children
5. Create hierarchical vector index

### Step 2: Ask Questions

Enter your legal research question in the main area. Examples:
- "What are the termination clauses in this contract?"
- "What is the governing law provision?"
- "Summarize the liability limitations"

### Step 3: Review Response

The system displays:
1. **AI Response**: The generated answer with citations
2. **Evidence Panel**: Source text chunks and PDF page images for verification

## ğŸ›¡ï¸ Security & Privacy

### Air-Gap Compliance

This application is designed for environments requiring complete data isolation:

- âœ… **No API Calls**: All inference runs locally via llama-cpp-python
- âœ… **No Telemetry**: No usage data collection or phone-home features
- âœ… **No External Dependencies at Runtime**: All models loaded from local disk
- âœ… **Memory-Only Processing**: Optional (default) in-memory vector storage
- âœ… **Source Verifiable**: All dependencies are open-source

### Suitable For

- HIPAA-regulated healthcare documents
- Attorney-client privileged communications
- ITAR/EAR controlled technical data
- Financial documents under SOX compliance
- Classified or sensitive government documents

## âš™ï¸ Configuration

### CPU Optimization Parameters

Located in `app.py`, the LLM configuration contains optimized parameters:

```python
n_threads=4       # Leave cores for OS (adjust based on your CPU)
n_batch=256       # Optimized for L3 cache
n_ctx=2048        # Context window (tokens)
temperature=0     # Deterministic output (critical for legal)
n_gpu_layers=0    # Force CPU-only execution
```

### Chunking Strategy (Parent-Child)

Optimized for legal documents:

```python
# Parent Chunks - Full semantic units
parent_chunk_size=1536   # ~1024-2048 tokens, captures full clauses

# Child Chunks - What gets indexed
child_chunk_size=384     # ~256 tokens, precise search
child_chunk_overlap=64   # Smooth transitions
```

## ğŸ—‚ï¸ Project Structure

```
LegalLocal-RAG/
â”œâ”€â”€ app.py              # Main application
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ README.md          # This file
â”œâ”€â”€ models/            # GGUF model files
â”‚   â””â”€â”€ Ministral-3-3B-Instruct-2512-Q4_K_M.gguf
â”œâ”€â”€ chroma_db/         # Vector store persistence (optional)
â”‚   â””â”€â”€ .gitkeep
â””â”€â”€ .cache/            # Embeddings cache
    â””â”€â”€ embeddings/
```

## ğŸ”§ Troubleshooting

### "Model file not found"

Ensure model file is placed in `/models` with exact filename:
- `Ministral-3-3B-Instruct-2512-Q4_K_M.gguf`

### "Out of memory"

- Close other applications
- Reduce `n_ctx` to 1024 in `app.py`
- Ministral 3B is already optimized for low memory (~2GB)

### Slow First Response

First query after model load includes:
- Model weight loading (~10-20 seconds)
- BGE embedding model initialization (~3-5 seconds)
- Vector index creation (~5-10 seconds per 100 pages)

Subsequent queries are much faster.

### PDF Processing Errors

- Ensure PDF is not password-protected
- Check PDF is not corrupted
- Scanned PDFs require OCR (not currently implemented)

## ğŸ”® Future Enhancements

- [ ] OCR support for scanned documents
- [ ] Multi-document analysis
- [ ] Regex-based structure detection for different legal formats
- [ ] Export to legal brief format
- [ ] Batch processing mode

## ğŸ“„ License

MIT License - See LICENSE file for details.

## âš ï¸ Disclaimer

This tool is for **research assistance only**. It does not constitute legal advice. Always verify AI-generated responses against original source documents. The developers assume no liability for decisions made based on this tool's output.

---

**Built for legal professionals who take privacy seriously.** âš–ï¸ğŸ”’
